{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNK0vQPVq5ywzqduBOHpbWx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Costumer Spening Analysis**\n","\n","In this project, we simulate collecting data from a business, storing in it Spark, and then using SparkSQL to get insight regarding specific requests from the client, which will be later used to analyze the data in order to find trends."],"metadata":{"id":"1DvEBBpsAPSB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vwWb9HmIByfW"},"outputs":[],"source":["!pip install pyspark"]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql import functions as F\n","from pyspark.sql.window import Window\n","\n","#Starting the spark session\n","spark = SparkSession.builder.appName(\"Costumer Sales Analysis\").getOrCreate() #initialized a Spark session\n","\n","#Simulating sales data, defining columns, and creating its Spark dataframe\n","sales_data = [\n","    (\"2024-01-15\",1,1,10,20.0,1),\n","    (\"2024-03-14\",2,4,23,25.0,2),\n","    (\"2024-04-15\",3,2,30,15.0,3),\n","    (\"2024-05-16\",4,1,32,45.0,4),\n","    (\"2024-07-19\",5,4,15,22.0,5),\n","]\n","\n","sales_schema = [\"Order_date\", \"Order_id\", \"Costumer_id\", \"Quantity_sold\", \"Price\", \"Product_Id\"]\n","sales_df = spark.createDataFrame(sales_data, sales_schema)\n","sales_df.createOrReplaceTempView(\"Sales\")\n","\n","#Simulating costumer data, defining columns, and creating its Spark dataframe\n","costumer_data = [\n","    (1, \"Costumer_A\"),\n","    (2, \"Costumer_B\"),\n","    (3, \"Costumer_C\"),\n","    (4, \"Costumer_D\"),\n","]\n","\n","costumer_data_schema = [\"Costumer_ID\", \"Costumer_Name\"]\n","costumer_data_df = spark.createDataFrame(costumer_data, costumer_data_schema)\n","costumer_data_df.createOrReplaceTempView(\"Costumers\")\n","\n","#Simulating products data, defining columns, and creating its Spark dataframe\n","products_data = [\n","    (1, \"Product_A\",\"Electronics\"),\n","    (2, \"Product_B\",\"Furniture\"),\n","    (3, \"Product_C\",\"Electronics\"),\n","    (4, \"Product_D\",\"Home_Appliances\"),\n","    (5, \"Product_E\", \"Furniture\"),\n","]\n","\n","product_schema = [\"Product_ID\", \"Product_Name\", \"Product_Category\"]\n","product_data_df = spark.createDataFrame(products_data, product_schema)\n","product_data_df.createOrReplaceTempView(\"Products\")\n","\n","#Simulating caterogies data, defining columns, and creating its Spark dataframe\n","categories_data = [\n","    (\"Electronics\", \"High-Tech _Gadgets\"),\n","    (\"Furniture\", \"Household_Items\"),\n","    (\"Home_Appliances\",\"Utility_Goods\")\n","]\n","\n","categories_data_schema = [\"Category\", \"Category_Description\"]\n","categories_data_df = spark.createDataFrame(categories_data, categories_data_schema)\n","categories_data_df.createOrReplaceTempView(\"Categories\")\n","\n","#Joining tables\n","joined_data = spark.sql(\"\"\"\n","  SELECT s.Order_date, s.Quantity_sold, s.Price,\n","  c.Costumer_Name,\n","  p.Product_Name, p.Product_Category,\n","  (s.Quantity_sold * s.Price) AS Total_Spent\n","  FROM Sales s\n","  JOIN Costumers c ON s.Costumer_Id = c.Costumer_Id\n","  JOIN Products p ON s.Product_Id = p.Product_Id\n","  JOIN Categories cat ON p.Product_Category = cat.category\n","\"\"\")\n","joined_data.createOrReplaceTempView(\"Joined_Sales\")\n","\n","#Caulculate total spending\n","costumer_spending = spark.sql(\"\"\"\n","  SELECT Costumer_Name, SUM(Total_Spent) AS Total_spending\n","  FROM Joined_Sales\n","  GROUP BY Costumer_Name\n","  ORDER BY Total_spending DESC\n","\"\"\")\n","\n","joined_data.createOrReplaceTempView(\"Joined_Sales\")\n","costumer_spending.show()\n","\n","#Top products for each category\n","top_products_by_category = spark.sql(\"\"\"\n","  SELECT *\n","  FROM(\n","    SELECT Product_Category, Product_Name, SUM(Quantity_Sold) AS Total_Quantity_Sold,\n","      ROW_NUMBER() OVER (PARTITION BY Product_Category ORDER BY SUM(Quantity_Sold) DESC) AS rank\n","    From Joined_Sales\n","    Group BY Product_Category, Product_Name\n","    ) ranked_products\n","    WHERE rank = 1\n","\"\"\")\n","top_products_by_category.show()\n","top_products_by_category.createOrReplaceTempView(\"Top_Products\")\n","\n","#Filter to show only results for the 'Electronics' category\n","electronics_data = spark.sql(\"\"\"\n","    SELECT Costumer_Name, Product_Name, Total_Spent\n","    FROM Joined_Sales\n","    WHERE Product_Category = 'Electronics'\n","    ORDER BY Total_Spent DESC\n","\"\"\")\n","electronics_data.show()\n","\n","\n","#Stopping Spark Session\n","spark.stop()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8N_HOf9pB0RZ","executionInfo":{"status":"ok","timestamp":1729311455141,"user_tz":300,"elapsed":46747,"user":{"displayName":"diego carrasco","userId":"14275717955758117669"}},"outputId":"b04c2787-ef01-44cc-d255-33d05ca17857"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------------+--------------+\n","|Costumer_Name|Total_spending|\n","+-------------+--------------+\n","|   Costumer_A|        1640.0|\n","|   Costumer_D|         905.0|\n","|   Costumer_B|         450.0|\n","+-------------+--------------+\n","\n","+----------------+------------+-------------------+----+\n","|Product_Category|Product_Name|Total_Quantity_Sold|rank|\n","+----------------+------------+-------------------+----+\n","|     Electronics|   Product_C|                 30|   1|\n","|       Furniture|   Product_B|                 23|   1|\n","| Home_Appliances|   Product_D|                 32|   1|\n","+----------------+------------+-------------------+----+\n","\n","+-------------+------------+-----------+\n","|Costumer_Name|Product_Name|Total_Spent|\n","+-------------+------------+-----------+\n","|   Costumer_B|   Product_C|      450.0|\n","|   Costumer_A|   Product_A|      200.0|\n","+-------------+------------+-----------+\n","\n"]}]}]}